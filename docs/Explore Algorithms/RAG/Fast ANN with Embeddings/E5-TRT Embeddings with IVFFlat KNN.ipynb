{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6166efcb-b7f8-424b-8015-cb646a764271",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Embedding Text with local (per node) NVIDIA TensorRT accelerator and GPU based Aproximate Nearest Neighbor (ANN)\n",
    "\n",
    "The demo extending existing [Azure OpenAI based demo](https://github.com/microsoft/SynapseML/blob/master/docs/Explore%20Algorithms/OpenAI/Quickstart%20-%20OpenAI%20Embedding%20and%20GPU%20based%20KNN.ipynb) when encoding is processed by OpenAI requests and KNN was using GPU based brute force search. This tutorial shows how to perform fast local embeddings using [multilingual E5 text embeddings](https://arxiv.org/abs/2402.05672) and fast aproximate Nearest Neighbor search using IVFFlat alcorithm. All tutorial stages accelerated by NVIDIA GPU using [NVIDIA TensorRT](https://developer.nvidia.com/tensorrt) and [Spark Rapids ML](https://github.com/NVIDIA/spark-rapids-ml). The tutorial folder contains two benchmark notebooks to demonstrate advantages of the presented GPU based approach compare to [previos CPU based demo](https://github.com/microsoft/SynapseML/blob/master/docs/Explore%20Algorithms/OpenAI/Quickstart%20-%20OpenAI%20Embedding.ipynb)\n",
    "\n",
    "The key prerequisites for this quickstart include a working Azure OpenAI resource, and an Apache Spark cluster with SynapseML installed. We suggest creating a Synapse workspace, but currently the notebook was running on Databricks GPU based cluster using Standard_NC24ads_A100_v4 with 6 workers. Databricks Runtime was 13.3 LTS ML (includes Apache Spark 3.4.1, GPU, Scala 2.12) with related [init_script](https://github.com/microsoft/SynapseML/tree/master/tools/init_scripts) to install all required packages.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0444a03d-a701-4f59-b1a1-c4addb797d07",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Step 1: Prepare Environment\n",
    "\n",
    "It will imports required libraries and get initial settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d188d8ee-8913-4170-8d35-8490f833ae95",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.10/site-packages/tritonclient/grpc/__init__.py:56: UserWarning: Imported version of grpc is 1.51.0. There is a memory leak in certain Python GRPC versions (1.43.0 to be specific). Please use versions <1.43.0 or >=1.51.1 to avoid leaks (see https://github.com/grpc/grpc/issues/28513).\n  warnings.warn(\nINFO:py4j.clientserver:Received command c on object id p0\nINFO:py4j.clientserver:Received command c on object id p0\nINFO:py4j.clientserver:Received command c on object id p0\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os \n",
    "import pyspark.sql.functions as F\n",
    "import model_navigator as nav\n",
    "import pandas as pd\n",
    "import torch\n",
    "import mlflow\n",
    "import datetime\n",
    "import pytz\n",
    "\n",
    "from model_navigator.inplace.config import DEFAULT_CACHE_DIR\n",
    "from stnavigator import SentenceTransformerNavigator\n",
    "from pyspark.sql.functions import pandas_udf, col, struct, arrays_zip, explode, broadcast\n",
    "from pyspark.sql.types import ArrayType, IntegerType, FloatType, StructType, StructField, StringType\n",
    "from pyspark.ml.functions import predict_batch_udf\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from spark_rapids_ml.knn import ApproximateNearestNeighbors, ApproximateNearestNeighborsModel\n",
    "# from data_loader import load_input_data\n",
    "from utility import generate_1M_data\n",
    "logging.getLogger('py4j').setLevel(logging.ERROR)\n",
    "mlflow.autolog(disable=True)\n",
    "# Define the PST timezone\n",
    "pst_timezone = pytz.timezone('US/Pacific')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22603456-4f44-4b3a-9751-9ca5231b799b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Step 2: Load Data\n",
    "\n",
    "In this demo we will explore a dataset of fine food reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0065427-a0d5-4867-8209-61969dc48082",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Start demo run 100\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/usr/lib/python3.10/logging/__init__.py\", line 1100, in emit\n    msg = self.format(record)\n  File \"/usr/lib/python3.10/logging/__init__.py\", line 943, in format\n    return fmt.format(record)\n  File \"/usr/lib/python3.10/logging/__init__.py\", line 678, in format\n    record.message = record.getMessage()\n  File \"/usr/lib/python3.10/logging/__init__.py\", line 368, in getMessage\n    msg = msg % self.args\nTypeError: not all arguments converted during string formatting\nCall stack:\n  File \"/databricks/python_shell/scripts/db_ipykernel_launcher.py\", line 141, in <module>\n    app.start()\n  File \"/databricks/python/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 712, in start\n    self.io_loop.start()\n  File \"/databricks/python/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 199, in start\n    self.asyncio_loop.run_forever()\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n    self._run_once()\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n    handle._run()\n  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/databricks/python/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n    await self.process_one()\n  File \"/databricks/python/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 499, in process_one\n    await dispatch(*args)\n  File \"/databricks/python/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n    await result\n  File \"/databricks/python/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n    reply_content = await reply_content\n  File \"/databricks/python_shell/dbruntime/DatabricksShell.py\", line 93, in do_execute\n    reply_content = await super().do_execute(*args, **kwargs)\n  File \"/databricks/python/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n    res = shell.run_cell(\n  File \"/databricks/python/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n    return super().run_cell(*args, **kwargs)\n  File \"/databricks/python/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 2961, in run_cell\n    result = self._run_cell(\n  File \"/databricks/python/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3016, in _run_cell\n    result = runner(coro)\n  File \"/databricks/python/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/databricks/python/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3221, in run_cell_async\n    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n  File \"/databricks/python/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3400, in run_ast_nodes\n    if await self.run_code(code, result, async_=asy):\n  File \"/databricks/python/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3460, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/root/.ipykernel/3875/command-1737443740659132-1445950604\", line 9, in <module>\n    logging.info(\"Current time in PST:\", current_time_pst.strftime('%Y-%m-%d %H:%M:%S %Z%z'))\nMessage: 'Current time in PST:'\nArguments: ('2024-05-29 15:11:52 PDT-0700',)\n"
     ]
    }
   ],
   "source": [
    "limit = 100\n",
    "logging.info(f\"Start demo run {limit}\")\n",
    "\n",
    "# Get the current time in UTC and convert it to PST\n",
    "current_start_time_utc = datetime.datetime.now(pytz.utc)\n",
    "current_time_pst = current_start_time_utc.astimezone(pst_timezone)\n",
    "\n",
    "# Print the current time in PST\n",
    "logging.info(\"Current time in PST:\", current_time_pst.strftime('%Y-%m-%d %H:%M:%S %Z%z'))\n",
    "\n",
    "df = generate_1M_data(spark, \"wasbs://publicwasb@mmlspark.blob.core.windows.net/fine_food_reviews_1k.csv\")\n",
    "\n",
    "df = df.limit(limit).repartition(10).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c69ee56-172f-413b-a335-d15482fda55e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Step 3: Generate Embeddings\n",
    "\n",
    "We will first generate embeddings using NVIDIA TensorRT optimized SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af3600ae-a156-477f-96aa-d1019dbfc8fa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def predict_batch_fn():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model = SentenceTransformerNavigator(\"intfloat/e5-large-v2\").eval()\n",
    "    model = nav.Module(model, name=\"e5-large-v2\")\n",
    "    model = model.to(device)\n",
    "    nav.load_optimized()\n",
    "\n",
    "    def predict(inputs):\n",
    "        with torch.no_grad():\n",
    "            output = model.encode(inputs.tolist(), convert_to_tensor=False, show_progress_bar=True)\n",
    "        return output\n",
    "    return predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c35c30d-7ee3-49cf-8a2e-13492c98e6fe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "encode = predict_batch_udf(predict_batch_fn,\n",
    "                           return_type=ArrayType(FloatType()),\n",
    "                           batch_size=10)\n",
    "\n",
    "# first pass caches model/fn\n",
    "all_embeddings = df.withColumn(\"embeddings\", encode(struct(\"combined\"))).cache()                                                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6885033f-6eea-4338-a632-2837582d91a1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Step 4: Build the query against embeddings\n",
    "\n",
    "Get query embeddings running standard SentenceTransformer just on the driver. Convert embedding results to a data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9141f44-63a0-460c-8b1d-139d0f72abe7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: intfloat/e5-large-v2\nINFO:sentence_transformers.SentenceTransformer:Use pytorch device: cuda\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48ccddb10d864b71a2320e0448fe6ec5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "if 'model' not in globals():\n",
    "    model = SentenceTransformer(\"intfloat/e5-large-v2\").eval()\n",
    "\n",
    "# Generate embeddings\n",
    "with torch.no_grad():\n",
    "    query = [\"desserts\", \"disgusting\"]\n",
    "    embeddings = [embedding.tolist() for embedding in model.encode(query)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "813a4a6d-f9a5-402f-a330-f27bd5af0218",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Prepare data including IDs\n",
    "data_with_ids = [(1, query[0], embeddings[0]), (2, query[1], embeddings[1])]\n",
    "\n",
    "# Define the schema for the DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), nullable=False),\n",
    "    StructField(\"query\", StringType(), nullable=False),\n",
    "    StructField(\"embeddings\", ArrayType(FloatType(), containsNull=False), nullable=False)\n",
    "])\n",
    "\n",
    "# Create a DataFrame using the data with IDs and the schema\n",
    "query_embeddings = spark.createDataFrame(data=data_with_ids, schema=schema).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0154ce06-5875-4236-8178-030d45091445",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Step 5: Build a fast vector index to over review embeddings\n",
    "\n",
    "We will use fast NVIDIA Rapids indexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa0e4178-75e4-412b-940e-25d55b7396ce",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rapids_knn = ApproximateNearestNeighbors(k=10)\n",
    "rapids_knn.setInputCol(\"embeddings\").setIdCol(\"id\")\n",
    "\n",
    "rapids_knn_model = rapids_knn.fit(all_embeddings.select(\"id\", \"embeddings\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "521c9c8e-6422-49c7-95f3-6bca44a90cbb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Step 6: Find top k Nearest Neighbors\n",
    "\n",
    "We will use fast ANN IVFFlat algorithm from Rapids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0fb3f3d7-bbb6-4105-bb86-b08fabba4ca4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(_, _, knn_df) = rapids_knn_model.kneighbors(query_embeddings.select(\"id\", \"embeddings\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c3cb089-64fd-4089-b45c-4d1671d2c500",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>query_id</th><th>indices</th><th>distances</th></tr></thead><tbody><tr><td>1</td><td>List(47, 77, 16, 36, 58, 30, 40, 84, 96, 25)</td><td>List(0.67687565, 0.6774632, 0.68367195, 0.68439406, 0.68577915, 0.68736494, 0.6878976, 0.69160885, 0.695291, 0.6957457)</td></tr><tr><td>2</td><td>List(67, 58, 75, 83, 98, 77, 30, 32, 40, 16)</td><td>List(0.6797807, 0.6797807, 0.68125886, 0.6842549, 0.69774866, 0.7001144, 0.7020483, 0.70247746, 0.70598215, 0.7062124)</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         [
          47,
          77,
          16,
          36,
          58,
          30,
          40,
          84,
          96,
          25
         ],
         [
          0.67687565,
          0.6774632,
          0.68367195,
          0.68439406,
          0.68577915,
          0.68736494,
          0.6878976,
          0.69160885,
          0.695291,
          0.6957457
         ]
        ],
        [
         2,
         [
          67,
          58,
          75,
          83,
          98,
          77,
          30,
          32,
          40,
          16
         ],
         [
          0.6797807,
          0.6797807,
          0.68125886,
          0.6842549,
          0.69774866,
          0.7001144,
          0.7020483,
          0.70247746,
          0.70598215,
          0.7062124
         ]
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "query_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "indices",
         "type": "{\"type\":\"array\",\"elementType\":\"long\",\"containsNull\":true}"
        },
        {
         "metadata": "{}",
         "name": "distances",
         "type": "{\"type\":\"array\",\"elementType\":\"float\",\"containsNull\":true}"
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(knn_df)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2,
    "widgetLayout": []
   },
   "notebookName": "E5-TRT Embeddings with IVFFlat KNN",
   "widgets": {}
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "save_output": true,
  "synapse_widget": {
   "state": {
    "4bd0e60b-98ae-4bfe-98ee-6f0399ceb456": {
     "persist_state": {
      "view": {
       "chartOptions": {
        "aggregationType": "count",
        "categoryFieldKeys": [
         "0"
        ],
        "chartType": "bar",
        "isStacked": false,
        "seriesFieldKeys": [
         "0"
        ]
       },
       "tableOptions": {},
       "type": "details"
      }
     },
     "sync_state": {
      "isSummary": false,
      "language": "scala",
      "table": {
       "rows": [
        {
         "0": "Once upon a time",
         "1": [
          " there was a girl who had a dream of becoming a writer.\n\nShe started writing short stories"
         ]
        },
        {
         "0": "Hello my name is",
         "1": [
          "***** and I have a question about my cat\n\nHello, thank you for bringing your question to"
         ]
        },
        {
         "0": "The best code is code thats",
         "1": [
          " not there\n\nCommenting your code is important. Not only does it help you remember what you"
         ]
        }
       ],
       "schema": [
        {
         "key": "0",
         "name": "prompt",
         "type": "string"
        },
        {
         "key": "1",
         "name": "text",
         "type": "ArrayType(StringType,true)"
        }
       ],
       "truncated": false
      }
     },
     "type": "Synapse.DataFrame"
    }
   },
   "version": "0.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
